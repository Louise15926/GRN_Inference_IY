{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Execution\n",
    "This file is a file to run the whole pipeline according to discussion on Dec. 12\n",
    "<br>\n",
    "Procedure:\n",
    "1. Pre-process data: Only use TF that are in gold standard, as discussed in class, prepare scores df\n",
    "2. For each model, predict GRN edges (with scores), using optimum hyperparameter, then standardize weights, compile each score to a 'score' table, with columns sources of score (including uniform scores from random clf) and index the edges\n",
    "3. Visualize PRC and ROC for each method for full nv dataframe\n",
    "4. Statistically examine each method by randomly selecting target edges, getting the mean auprc and stdev, conduct tests (vs random t test, anova, tukey if anove rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import grn_preds\n",
    "\n",
    "# Read in expression, tf list, and gold files\n",
    "nv_df = pd.read_csv('../dataset/yeast_networks/expression/NatVar.txt', sep='\\t').set_index('Gene')\n",
    "nv_tf = pd.read_csv('../dataset/yeast_networks/expression/NatVar_TF_names.txt', sep='\\t', header=None)\n",
    "nv_tf.columns = ['TF']\n",
    "\n",
    "nv_gold = grn_preds.generate_gold_dataset(['../dataset/yeast_networks/gold/MacIsaac2.NatVar.txt', \n",
    "                                            '../dataset/yeast_networks/gold/YEASTRACT_Count3.NatVar.txt',\n",
    "                                            '../dataset/yeast_networks/gold/YEASTRACT_Type2.NatVar.txt'])\n",
    "\n",
    "# Filter tf list based on gold\n",
    "nv_tf = nv_tf[nv_tf.loc[:, 'TF'].isin(set(nv_gold.loc[:, 'Regulator']))]\n",
    "\n",
    "# Generate scores df\n",
    "nv_scores_df = grn_preds.generate_possible_edges(nv_tf.loc[:, 'TF'], nv_df)\n",
    "nv_scores_df = grn_preds.populate_actual_column(nv_scores_df, nv_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predict GRN edges with scores, using optimum hyperparameter or example hyperparameter, then store in compiled table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Predict GRN edges with optimum hyperparameter (Takes about 10-12 hours, make sure back this file up!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for target in nv_df.index:\n",
    "    lasso_edges, lasso_scores = grn_preds.grn_lasso(target, nv_tf.loc[:, 'TF'], nv_df, alphas = [0.001, 0.01, 0.1, 1, 10, 100], cv = 5)\n",
    "    rf_edges, rf_scores = grn_preds.grn_regforest(target, nv_tf.loc[:, 'TF'], nv_df, n_estimators = 100, max_depth = 8, bootstrap = True, min_samples_leaf = 10, n_jobs = -1)\n",
    "    svr_edges, svr_scores = grn_preds.grn_linear_svr(target, nv_tf.loc[:, 'TF'], nv_df)\n",
    "    \n",
    "    nv_scores_df.loc[lasso_edges, 'Lasso scores'] = abs(lasso_scores)\n",
    "    nv_scores_df.loc[rf_edges, 'Regforest scores'] = rf_scores\n",
    "    nv_scores_df.loc[svr_edges, 'SVR scores'] = abs(svr_scores)\n",
    "    \n",
    "nv_scores_df.to_csv('../dataset/nv_scores_df.csv')\n",
    "'''\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Standardize weights so it has mean 0 and standard deviation 1, then add each weight with `abs(min(weight))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Load nv_scores_df\n",
    "\n",
    "nv_scores_df = pd.read_csv('../dataset/nv_scores_df.csv', index_col=0)\n",
    "# Standardize scores for each scoring methods\n",
    "methods = [\"Lasso\", \"Regforest\", \"SVR\"]\n",
    "i = 0\n",
    "print(len(set(nv_scores_df.loc[:, 'Target'])))\n",
    "for method in methods:\n",
    "    nv_scores_df.loc[:, f'Standardized {method} scores'] = 0\n",
    "\n",
    "for target in list(set(nv_scores_df.loc[:, 'Target'])):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    for method in methods:\n",
    "        new_index, standardized_scores = grn_preds.standardize_scores(target, nv_scores_df, method)\n",
    "        nv_scores_df.loc[new_index, f'Standardized {method} scores'] = standardized_scores\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "nv_scores_df.to_csv('../dataset/nv_scores_df_stand.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv_scores_df_stand = pd.read_csv('../dataset/nv_scores_df_stand.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove negative scores from weights in standardized scores by adding them with the smallest absolute value\n",
    "nv_scores_df_stand.loc[:, 'Standardized Lasso scores'] = nv_scores_df_stand.loc[:, 'Standardized Lasso scores'] + abs(min(nv_scores_df_stand.loc[:, 'Standardized Lasso scores']))\n",
    "nv_scores_df_stand.loc[:, 'Standardized Regforest scores'] = nv_scores_df_stand.loc[:, 'Standardized Regforest scores'] + abs(min(nv_scores_df_stand.loc[:, 'Standardized Regforest scores']))\n",
    "nv_scores_df_stand.loc[:, 'Standardized SVR scores'] = nv_scores_df_stand.loc[:, 'Standardized SVR scores'] + abs(min(nv_scores_df_stand.loc[:, 'Standardized SVR scores']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize PRC and ROC for each method for full nv dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve\n",
    "\n",
    "# Get fpr, tpr stat for different method\n",
    "roc_dic = {'raw': {}, 'stand': {}}\n",
    "prc_dic = {'raw': {}, 'stand': {}}\n",
    "\n",
    "for method in ['Lasso', 'Regforest', 'SVR']:\n",
    "    # Prep labeling so code is easy to follow\n",
    "    actual_edges = nv_scores_df_stand.loc[:, 'Actual']\n",
    "    raw_label = f'{method} scores'\n",
    "    stand_label = f'Standardized {method} scores'\n",
    "    \n",
    "    # Get ROC and PRC stats\n",
    "    raw_roc = roc_curve(actual_edges, nv_scores_df_stand.loc[:, raw_label])\n",
    "    stand_roc = roc_curve(actual_edges, nv_scores_df_stand.loc[:, stand_label])\n",
    "    raw_prc = precision_recall_curve(actual_edges, nv_scores_df_stand.loc[:, raw_label])\n",
    "    stand_prc = precision_recall_curve(actual_edges, nv_scores_df_stand.loc[:, stand_label])\n",
    "    \n",
    "    # Get AUC for each curve\n",
    "    auc_raw_roc = auc(raw_roc[0], raw_roc[1])\n",
    "    auc_stand_roc = auc(stand_roc[0], stand_roc[1])\n",
    "    auc_raw_prc = auc(raw_prc[1], raw_prc[0])\n",
    "    auc_stand_prc = auc(stand_prc[1], stand_prc[0])\n",
    "    \n",
    "    # Store to respective dictionary\n",
    "    roc_dic['raw'][raw_label] = (raw_roc[0], raw_roc[1], auc_raw_roc)\n",
    "    roc_dic['stand'][stand_label] = (stand_roc[0], stand_roc[1], auc_stand_roc)\n",
    "    prc_dic['raw'][raw_label] = (raw_prc[1], raw_prc[0], auc_raw_prc)\n",
    "    prc_dic['stand'][stand_label] = (stand_prc[1], stand_prc[0], auc_raw_prc)\n",
    "\n",
    "\n",
    "# Get stats for uniform scores\n",
    "\n",
    "## Get ROC and PRC stats\n",
    "uniform_roc = roc_curve(actual_edges, nv_scores_df_stand.loc[:, 'Uniform score'])\n",
    "uniform_prc = precision_recall_curve(actual_edges, nv_scores_df_stand.loc[:, 'Uniform score'])\n",
    "\n",
    "## Get AUC for each curve\n",
    "uniform_auc_roc = auc(uniform_roc[0], uniform_roc[1])\n",
    "uniform_auc_prc = auc(uniform_prc[1], uniform_prc[0])\n",
    "\n",
    "## Store to respective dictionary\n",
    "roc_dic['uniform score'] = (uniform_roc[0], uniform_roc[1], uniform_auc_roc)\n",
    "prc_dic['uniform score'] = (uniform_prc[1], uniform_prc[0], uniform_auc_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate curves manually\n",
    "raw_roc_fig = plt.figure(0, figsize=(10, 10))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Figure 1: Raw ROC Curve')\n",
    "\n",
    "colors = ['C1', 'C2', 'C3']\n",
    "col_idx = 0\n",
    "for method, stats in roc_dic['raw'].items():\n",
    "    plt.plot(stats[0], stats[1], color = colors[col_idx], lw = 2, label = f'{method[:-7]}: {str(stats[2])[:6]}')\n",
    "    col_idx += 1\n",
    "col_idx = 0\n",
    "\n",
    "plt.plot(roc_dic['uniform score'][0], roc_dic['uniform score'][1], color = 'black',\n",
    "         lw = 2, linestyle = '--', label = f'Uniform score: {roc_dic[\"uniform score\"][2]}')\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.savefig('../figures/fig_1_raw_roc.png', format='png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure for Standardized ROC\n",
    "\n",
    "stand_roc_fig = plt.figure(1, figsize=(10, 10))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Figure 2: Standardized ROC Curve')\n",
    "\n",
    "colors = ['C1', 'C2', 'C3']\n",
    "col_idx = 0\n",
    "for method, stats in roc_dic['stand'].items():\n",
    "    plt.plot(stats[0], stats[1], color = colors[col_idx], lw = 2, label = f'{method[:-7]}: {str(stats[2])[:6]}')\n",
    "    col_idx += 1\n",
    "col_idx = 0\n",
    "\n",
    "plt.plot(roc_dic['uniform score'][0], roc_dic['uniform score'][1], color = 'black',\n",
    "         lw = 2, linestyle = '--', label = f'Uniform score: {roc_dic[\"uniform score\"][2]}')\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.savefig('../figures/fig_2_stand_roc.png', format='png')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure for raw PRC\n",
    "raw_prc_fig = plt.figure(1, figsize=(10, 10))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Figure 3: Raw PRC Curve')\n",
    "\n",
    "colors = ['C1', 'C2', 'C3']\n",
    "col_idx = 0\n",
    "for method, stats in prc_dic['raw'].items():\n",
    "    plt.plot(stats[0], stats[1], color = colors[col_idx], lw = 2, label = f'{method[:-7]}: {str(stats[2])[:6]}')\n",
    "    col_idx += 1\n",
    "col_idx = 0\n",
    "\n",
    "# plt.plot(prc_dic['uniform score'][0], prc_dic['uniform score'][1], color = 'black',\n",
    "#          lw = 2, linestyle = '--', label = f'Uniform score: {prc_dic[\"uniform score\"][2]}')\n",
    "\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.savefig('../figures/fig_3_raw_prc.png', format='png')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure for Standardized PRC\n",
    "\n",
    "stand_prc_fig = plt.figure(1, figsize=(10, 10))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Figure 4: Precision PRC Curve')\n",
    "\n",
    "colors = ['C1', 'C2', 'C3']\n",
    "col_idx = 0\n",
    "for method, stats in prc_dic['stand'].items():\n",
    "    plt.plot(stats[0], stats[1], color = colors[col_idx], lw = 2, label = f'{method[:-7]}: {str(stats[2])[:6]}')\n",
    "    col_idx += 1\n",
    "col_idx = 0\n",
    "\n",
    "# plt.plot(prc_dic['uniform score'][0], prc_dic['uniform score'][1], color = 'black',\n",
    "#          lw = 2, linestyle = '--', label = f'Uniform score: {prc_dic[\"uniform score\"][2]}')\n",
    "\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.savefig('../figures/fig_4_stand_prc.png', format='png')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistically examine each method by randomly selecting target edges\n",
    "Getting the mean auprc and stdev, conduct tests (vs random t test, anova, tukey if anova rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# We select 100 target genes for each sampling, and we sample 100 times from the scores, then we get each method's auc for each sampling, for ROC and PRC.\n",
    "import random\n",
    "\n",
    "# Prepare score dictionary / list\n",
    "methods = ['Lasso', 'Regforest', 'SVR', 'Standardized Lasso', 'Standardized Regforest', 'Standardized SVR']\n",
    "score_dic = {'auroc': {}, 'auprc': {}}\n",
    "\n",
    "for method in methods:\n",
    "    score_dic['auroc'][method] = []\n",
    "    score_dic['auprc'][method] = []\n",
    "\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    sampled_targets = random.sample(list(set(nv_scores_df_stand.loc[:, 'Target'])), 100)\n",
    "    subset_df = nv_scores_df_stand[nv_scores_df_stand.loc[:, 'Target'].isin(sampled_targets)]\n",
    "\n",
    "    for method in methods:\n",
    "        method_auroc = grn_preds.generate_auroc(subset_df.loc[:, 'Actual'], subset_df.loc[:, f'{method} scores'], show = False)\n",
    "        method_auprc = grn_preds.generate_auprc(subset_df.loc[:, 'Actual'], subset_df.loc[:, f'{method} scores'], show = False)\n",
    "\n",
    "        score_dic['auroc'][method].append(method_auroc)\n",
    "        score_dic['auprc'][method].append(method_auprc)\n",
    "\n",
    "# Store to csv file\n",
    "auroc_df = pd.DataFrame.from_dict(score_dic['auroc'])\n",
    "auprc_df = pd.DataFrame.from_dict(score_dic['auprc'])\n",
    "\n",
    "auroc_df.to_csv('../dataset/auroc_sampling_scores.csv', index=False)\n",
    "auprc_df.to_csv('../dataset/auprc_sampling_scores.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load auroc and auprc sampling df\n",
    "auroc_df = pd.read_csv('../dataset/auroc_sampling_scores.csv')\n",
    "auprc_df = pd.read_csv('../dataset/auprc_sampling_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
